# Sparse_Attention_on_Transformer-based_model


This is the replication package and dataset for FSE'23 submission.
For the detailed repliation package, please go to: https://github.com/invisiblehead/Sparse_Attention_on_Transformer-based_model
For the processed dataset, please go to:
https://zenodo.org/deposit/7606184
Also, the original dataset without preprocessing can be found here: https://osf.io/d45bw/
